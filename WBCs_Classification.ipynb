{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WBCs Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA4ujV_HE7Gl"
      },
      "source": [
        "Import libraries & define paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sURJl1dCq9D"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import Model\r\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\r\n",
        "from tensorflow.keras.models import Sequential, load_model\r\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df-NGlikC4rH"
      },
      "source": [
        "## More directories paths defined\r\n",
        "train_dir = os.path.join(path, 'train')\r\n",
        "validation_dir = os.path.join(path, 'validation')\r\n",
        "test_dir = os.path.join(path, 'test')\r\n",
        "\r\n",
        "## Directory with training of WBCs datasets\r\n",
        "train_healthy_dir = os.path.join(train_dir, 'healthy')\r\n",
        "train_ALL_dir = os.path.join(train_dir, 'ALL')\r\n",
        "\r\n",
        "## Directory with validation of WBCs datasets\r\n",
        "validation_healthy_dir = os.path.join(validation_dir, 'healthy')\r\n",
        "validation_ALL_dir = os.path.join(validation_dir, 'ALL')\r\n",
        "\r\n",
        "## Directory with testing of WBCs datasets\r\n",
        "test_healthy_dir = os.path.join(test_dir, 'healthy')  \r\n",
        "test_ALL_dir = os.path.join(test_dir, 'ALL') \r\n",
        "\r\n",
        "## Batch size\r\n",
        "TRAIN_BATCH_SIZE = 16\r\n",
        "VALIDATION_BATCH_SIZE = 13\r\n",
        "TEST_BATCH_SIZE = 13\r\n",
        "\r\n",
        "## Input image width, height and shape\r\n",
        "IMG_HEIGHT = 150\r\n",
        "IMG_WIDTH = 150\r\n",
        "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)\r\n",
        "\r\n",
        "## Total number of different datasets\r\n",
        "num_training_healthy = len(os.listdir(train_healthy_dir))\r\n",
        "num_training_ALL = len(os.listdir(train_ALL_dir))\r\n",
        "\r\n",
        "num_validation_healthy = len(os.listdir(validation_healthy_dir))\r\n",
        "num_validation_ALL = len(os.listdir(validation_ALL_dir))\r\n",
        "\r\n",
        "num_test_healthy = len(os.listdir(test_healthy_dir))\r\n",
        "num_test_ALL = len(os.listdir(test_ALL_dir))\r\n",
        "\r\n",
        "total_train = num_training_ALL + num_training_healthy\r\n",
        "total_validation = num_validation_ALL + num_validation_healthy\r\n",
        "total_test = num_test_ALL + num_test_healthy\r\n",
        "\r\n",
        "print(\"Size of training dataset: \" , total_train)\r\n",
        "print(\"Size of validation dataset: \" , total_validation)\r\n",
        "print(\"Size of test dataset: \" , total_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jltu3FoqDBzW"
      },
      "source": [
        "Download pretrained InceptionV3 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIqrchU8C_fR"
      },
      "source": [
        "!wget --no-check-certificate \\\r\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\r\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYHtnGykDFlM"
      },
      "source": [
        "View architecture of InceptionV3 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1vUQltgDNj7"
      },
      "source": [
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\r\n",
        "\r\n",
        "pre_trained_model = InceptionV3(input_shape = IMG_SHAPE, \r\n",
        "                                include_top = False, \r\n",
        "                                weights = None)\r\n",
        "\r\n",
        "pre_trained_model.load_weights(local_weights_file)\r\n",
        "\r\n",
        "for layer in pre_trained_model.layers:\r\n",
        "    layer.trainable = False\r\n",
        "\r\n",
        "pre_trained_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0nu5Ga7DSEd"
      },
      "source": [
        "Display architecture of pretrained InceptionV3 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SctVoVmoDWSd"
      },
      "source": [
        "layers = [(layer, layer.name, layer.trainable) for layer in pre_trained_model.layers]\r\n",
        "pre_trained_model_architecture = pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\r\n",
        "pd.set_option('display.max_rows', pre_trained_model_architecture.shape[0]+1)\r\n",
        "pre_trained_model_architecture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXrpXI1LDW4h"
      },
      "source": [
        "Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZQqOqsVDbHZ"
      },
      "source": [
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\r\n",
        "\r\n",
        "def extract_features(directory, sample_count, batch_size):\r\n",
        "    features = np.zeros(shape=(sample_count, 3, 3, 2048))\r\n",
        "    labels = np.zeros(shape=(sample_count))\r\n",
        "\r\n",
        "    generator = datagen.flow_from_directory(directory,\r\n",
        "                                            target_size=(IMG_WIDTH, IMG_HEIGHT),\r\n",
        "                                            batch_size = batch_size,\r\n",
        "                                            class_mode='binary')\r\n",
        "    i = 0\r\n",
        "    for inputs_batch, labels_batch in generator:\r\n",
        "        features_batch = pre_trained_model.predict(inputs_batch)\r\n",
        "        features[i * batch_size: (i + 1) * batch_size] = features_batch\r\n",
        "        labels[i * batch_size: (i + 1) * batch_size] = labels_batch\r\n",
        "        i += 1\r\n",
        "        if i * batch_size >= sample_count:\r\n",
        "            break\r\n",
        "    return features, labels\r\n",
        "\r\n",
        "train_features, train_labels = extract_features(train_dir, total_train, TRAIN_BATCH_SIZE)  \r\n",
        "validation_features, validation_labels = extract_features(validation_dir, total_validation, VALIDATION_BATCH_SIZE)\r\n",
        "test_features, test_labels = extract_features(test_dir, total_test, TEST_BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZaHsKwWDgI4"
      },
      "source": [
        "Define new classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgX6ciorDiWF"
      },
      "source": [
        "new_classifier = Sequential([\r\n",
        "  GlobalAveragePooling2D(input_shape=(3, 3, 2048)),\r\n",
        "  Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "\r\n",
        "new_classifier.compile(\r\n",
        "    optimizer = Adam(lr=0.0001/10), \r\n",
        "    loss = BinaryCrossentropy(),\r\n",
        "    metrics = [BinaryAccuracy(), Precision(), Recall(), TruePositives(), TrueNegatives(), FalsePositives(), FalseNegatives()]\r\n",
        ")\r\n",
        "\r\n",
        "new_classifier.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNtux9APDj1b"
      },
      "source": [
        "Train newly defined classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U8UEL9LDnfP"
      },
      "source": [
        "checkpoint = ModelCheckpoint(\r\n",
        "        filepath=file_p\r\n",
        "        monitor='val_loss',\r\n",
        "        save_best_only=False,\r\n",
        "        save_weights_only=True,\r\n",
        "        verbose=1,\r\n",
        "        save_freq='epoch'\r\n",
        ")\r\n",
        "\r\n",
        "new_classifier_history = new_classifier.fit(\r\n",
        "                                train_features, train_labels,\r\n",
        "                                epochs=300,\r\n",
        "                                batch_size=TRAIN_BATCH_SIZE,\r\n",
        "                                steps_per_epoch = len(train_features) // TRAIN_BATCH_SIZE,\r\n",
        "                                verbose=2, \r\n",
        "                                callbacks = [checkpoint],\r\n",
        "                                validation_data=(validation_features, validation_labels),\r\n",
        "                                validation_steps = len(validation_features) // VALIDATION_BATCH_SIZE\r\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDLX8sA1DvZC"
      },
      "source": [
        "Visualize results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU046frIDx8A"
      },
      "source": [
        "## Visualize training results\r\n",
        "acc = new_classifier_history.history['binary_accuracy']\r\n",
        "val_acc = new_classifier_history.history['val_binary_accuracy']\r\n",
        "\r\n",
        "loss = new_classifier_history.history['loss']\r\n",
        "val_loss = new_classifier_history.history['val_loss']\r\n",
        "\r\n",
        "epochs_range = range(1,301)\r\n",
        "\r\n",
        "## Plot the graph of training and validation accuracy\r\n",
        "plt.figure(figsize=(16, 8))\r\n",
        "plt.subplot(1, 2, 1)\r\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\r\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\r\n",
        "plt.ylim(top=1.0) \r\n",
        "plt.ylim(bottom=0.0)\r\n",
        "plt.axvline(x=95, color='black')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "## Plot the graph of training and validation loss\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\r\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\r\n",
        "plt.ylim(top=2.0) \r\n",
        "plt.ylim(bottom=0.0)\r\n",
        "plt.axvline(x=95, color='black')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "## Best checkpoint\r\n",
        "'''\r\n",
        "\r\n",
        "Epoch 95/300\r\n",
        "13/13 - 0s - loss: 0.4906 - binary_accuracy: 0.8173 - precision_53: 0.8000 - recall_53: 0.8462 - true_positives_53: 88.0000 - true_negatives_53: 82.0000 - false_positives_53: 22.0000 - false_negatives_53: 16.0000 - \r\n",
        "val_loss: 0.5274 - val_binary_accuracy: 0.8077 - val_precision_53: 0.7857 - val_recall_53: 0.8462 - val_true_positives_53: 11.0000 - val_true_negatives_53: 10.0000 - val_false_positives_53: 3.0000 - val_false_negatives_53: 2.0000\r\n",
        "\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbhYZfikD1pP"
      },
      "source": [
        "Evaluate new classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KMkUcHCD3aU"
      },
      "source": [
        "## Define architecture of model\r\n",
        "final_classifier = Sequential([\r\n",
        "  GlobalAveragePooling2D(input_shape=(3, 3, 2048)),\r\n",
        "  Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "\r\n",
        "## Load selected trained weights of classifier\r\n",
        "final_classifier.load_weights(load_weights_path)\r\n",
        "\r\n",
        "## Freeze all layers\r\n",
        "final_classifier.trainable = False\r\n",
        "\r\n",
        "## Compile the new classifier\r\n",
        "final_classifier.compile(\r\n",
        "    optimizer = Adam(lr=0.0001/10), \r\n",
        "    loss = BinaryCrossentropy(),\r\n",
        "    metrics = [BinaryAccuracy(), Precision(), Recall(), TruePositives(), TrueNegatives(), FalsePositives(), FalseNegatives()]\r\n",
        ")\r\n",
        "\r\n",
        "final_classifier.summary()\r\n",
        "\r\n",
        "# Carry out testing of the classifier\r\n",
        "test_history = final_classifier.evaluate(\r\n",
        "    test_features, test_labels,\r\n",
        "    batch_size=TEST_BATCH_SIZE,\r\n",
        "    verbose=1,\r\n",
        "    steps=len(test_features) // TEST_BATCH_SIZE\r\n",
        ")\r\n",
        "\r\n",
        "# Evaluation results:\r\n",
        "# loss: 0.4982 - binary_accuracy: 0.8974 - precision_1: 0.9444 - recall_1: 0.8308 - true_positives_1: 8.6667 - \r\n",
        "# true_negatives_1: 10.6667 - false_positives_1: 0.6667 - false_negatives_1: 1.6667"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5yfgSDLD7UO"
      },
      "source": [
        "Model Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Obw5ZwzEAGj"
      },
      "source": [
        "train_val_image_gen = ImageDataGenerator(\r\n",
        "                    rotation_range=180,\r\n",
        "                    horizontal_flip=True,\r\n",
        "                    vertical_flip=True,\r\n",
        "                    preprocessing_function=preprocess_input\r\n",
        "                  )\r\n",
        "\r\n",
        "test_image_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\r\n",
        "\r\n",
        "train_data_gen = train_val_image_gen.flow_from_directory(batch_size=TRAIN_BATCH_SIZE, \r\n",
        "                                                     directory=train_dir, \r\n",
        "                                                     shuffle=True, \r\n",
        "                                                     target_size=(IMG_WIDTH, IMG_HEIGHT), \r\n",
        "                                                     class_mode='binary')\r\n",
        "\r\n",
        "validation_data_gen = train_val_image_gen.flow_from_directory(batch_size=VALIDATION_BATCH_SIZE, \r\n",
        "                                                          directory=validation_dir, \r\n",
        "                                                          shuffle=True, \r\n",
        "                                                          target_size=(IMG_WIDTH, IMG_HEIGHT), \r\n",
        "                                                          class_mode='binary')\r\n",
        "\r\n",
        "test_data_gen = test_image_gen.flow_from_directory(batch_size=TEST_BATCH_SIZE,\r\n",
        "                                                              directory=test_dir,\r\n",
        "                                                              target_size=(IMG_WIDTH, IMG_HEIGHT), \r\n",
        "                                                              class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r0nZpU9EE3i"
      },
      "source": [
        "Build final classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTX0B51XECOj"
      },
      "source": [
        "final_classifier = Sequential([\r\n",
        "  GlobalAveragePooling2D(input_shape=(3, 3, 2048)),\r\n",
        "  Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "\r\n",
        "final_classifier.load_weights(final_load_weights_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpl33dyYEKFZ"
      },
      "source": [
        "## Pre-trained InceptionV3 as based model\r\n",
        "based_model = InceptionV3(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\r\n",
        "\r\n",
        "## Unfreeze all layers of InceptionV3\r\n",
        "based_model.trainable = True\r\n",
        "\r\n",
        "## Refreeze layers until layers plan to fine-tune \r\n",
        "for layer in based_model.layers[:268]:\r\n",
        "  layer.trainable = False\r\n",
        "\r\n",
        "## Display the architecture of InceptionV3\r\n",
        "layers = [(layer, layer.name, layer.trainable) for layer in based_model.layers]\r\n",
        "based_model_architecture = pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\r\n",
        "pd.set_option('display.max_rows', based_model_architecture.shape[0]+1)\r\n",
        "based_model_architecture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbU371CZEOn9"
      },
      "source": [
        "Define final classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMv5Z_MNEL-h"
      },
      "source": [
        "model = Sequential([\r\n",
        "  based_model,\r\n",
        "  final_classifier\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(\r\n",
        "    optimizer = Adam(lr=0.0001/10), \r\n",
        "    loss = BinaryCrossentropy(),\r\n",
        "    metrics = [BinaryAccuracy(), Precision(), Recall(), TruePositives(), TrueNegatives(), FalsePositives(), FalseNegatives()]\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0cGz-wPERH2"
      },
      "source": [
        "Train final classifier model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6ypct9aET54"
      },
      "source": [
        "checkpoint = ModelCheckpoint(\r\n",
        "        filepath=file_p,\r\n",
        "        monitor='val_loss',\r\n",
        "        save_best_only=False,\r\n",
        "        save_weights_only=False,\r\n",
        "        verbose=1,\r\n",
        "        save_freq='epoch'\r\n",
        ")\r\n",
        "\r\n",
        "model_history = model.fit(\r\n",
        "                    train_data_gen,\r\n",
        "                    epochs = 100,\r\n",
        "                    steps_per_epoch = train_data_gen.samples // TRAIN_BATCH_SIZE,\r\n",
        "                    verbose = 2, \r\n",
        "                    callbacks = [checkpoint],\r\n",
        "                    validation_data = validation_data_gen,\r\n",
        "                    validation_steps = validation_data_gen.samples // VALIDATION_BATCH_SIZE\r\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfYcTnkPEZaB"
      },
      "source": [
        "Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnnmefPcEbnA"
      },
      "source": [
        "acc = model_history.history['binary_accuracy']\r\n",
        "val_acc = model_history.history['val_binary_accuracy']\r\n",
        "\r\n",
        "loss = model_history.history['loss']\r\n",
        "val_loss = model_history.history['val_loss']\r\n",
        "\r\n",
        "epochs_range = range(1,101)\r\n",
        "\r\n",
        "## Plot the graph of training and validation accuracy\r\n",
        "plt.figure(figsize=(16, 8))\r\n",
        "plt.subplot(1, 2, 1)\r\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\r\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\r\n",
        "plt.ylim(top=1.0) \r\n",
        "plt.ylim(bottom=0.0)\r\n",
        "plt.axvline(x=20, color='black')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "## Plot the graph of training and validation loss\r\n",
        "plt.subplot(1, 2, 2)\r\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\r\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\r\n",
        "plt.ylim(top=2.0) \r\n",
        "plt.ylim(bottom=0.0)\r\n",
        "plt.axvline(x=20, color='black')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "## Best Checkpoint\r\n",
        "'''\r\n",
        "\r\n",
        "Epoch 20/100\r\n",
        "13/13 - 14s - loss: 0.2028 - binary_accuracy: 0.9327 - precision_2: 0.9412 - recall_2: 0.9231 - true_positives_2: 96.0000 - true_negatives_2: 98.0000 - false_positives_2: 6.0000 - false_negatives_2: 8.0000 - \r\n",
        "val_loss: 0.2941 - val_binary_accuracy: 0.9231 - val_precision_2: 0.9231 - val_recall_2: 0.9231 - val_true_positives_2: 12.0000 - val_true_negatives_2: 12.0000 - val_false_positives_2: 1.0000 - val_false_negatives_2: 1.0000\r\n",
        "\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKn8-4-WEenz"
      },
      "source": [
        "Evaluate Finetuned classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxIclOhjEjeq"
      },
      "source": [
        "from tensorflow.keras.models import load_model\r\n",
        "\r\n",
        "final_model = load_model(final_weights_path)\r\n",
        "final_model.trainable = False\r\n",
        "\r\n",
        "final_model.compile(\r\n",
        "    optimizer = Adam(lr=0.0001/10),\r\n",
        "    loss = BinaryCrossentropy(),\r\n",
        "    metrics = [BinaryAccuracy(), Precision(), Recall(), TruePositives(), TrueNegatives(), FalsePositives(), FalseNegatives()]\r\n",
        ")\r\n",
        "\r\n",
        "test_history = final_model.evaluate(\r\n",
        "    test_data_gen,\r\n",
        "    verbose=1,\r\n",
        "    steps=test_data_gen.samples // TEST_BATCH_SIZE\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soR1PxD9Emb9"
      },
      "source": [
        "Testing final classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POMnf81vEl1n"
      },
      "source": [
        "test_history = final_model.evaluate(\r\n",
        "    test_data_gen,\r\n",
        "    verbose=1,\r\n",
        "    steps=test_data_gen.samples // TEST_BATCH_SIZE\r\n",
        ")\r\n",
        "\r\n",
        "#Evaluation Results:\r\n",
        "#loss: 0.2229 - binary_accuracy: 0.9615 - precision_4: 1.0000 - recall_4: 0.9231 - \r\n",
        "#true_positives_4: 12.0000 - true_negatives_4: 13.0000 - false_positives_4: 0.0000e+00 - false_negatives_4: 1.0000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0k_dl78Eq2x"
      },
      "source": [
        "Sample classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsuSU56fEtq3"
      },
      "source": [
        "pred_dir = os.path.join(path, 'prediction')\r\n",
        "\r\n",
        "predict_data_gen = test_image_gen.flow_from_directory(batch_size=1,\r\n",
        "                                                        directory=pred_dir,\r\n",
        "                                                        target_size=(IMG_WIDTH, IMG_HEIGHT), \r\n",
        "                                                        class_mode='binary')\r\n",
        "\r\n",
        "fnames = predict_data_gen.filenames\r\n",
        "ground_truth = predict_data_gen.classes\r\n",
        "label2index = predict_data_gen.class_indices\r\n",
        "idx2label = dict((v,k) for k,v in label2index.items())\r\n",
        "\r\n",
        "print(fnames)\r\n",
        "print(ground_truth)\r\n",
        "print(label2index)\r\n",
        "print(idx2label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js3ZDyqzEuaB"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\r\n",
        "\r\n",
        "predictions = final_model.predict(predict_data_gen)\r\n",
        "\r\n",
        "for i in range(len(predictions)):\r\n",
        "    pred_label = idx2label[int(predictions[i] + 0.5)]\r\n",
        "    actual_label = idx2label[ground_truth[i]]\r\n",
        "    \r\n",
        "    title = '{}, Prediction: {}, Actual label: {}, Predicted label: {}'.format(fnames[i], predictions[i], actual_label, pred_label)\r\n",
        "    \r\n",
        "    actual_img = image.load_img('{}/{}'.format(test_dir,fnames[i]))\r\n",
        "    plt.figure(figsize=[5,5])\r\n",
        "    plt.axis('off')\r\n",
        "    plt.title(title)\r\n",
        "    plt.imshow(actual_img)\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}